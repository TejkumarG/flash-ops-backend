# Flash-Ops: Natural Language to SQL System

Production-ready NL2SQL system supporting **1 crore (10 million) tables** with intelligent table selection and query generation.

## ğŸš€ Features

- âœ… **Massive Scale**: Handles 1 crore tables using FAISS vector search
- âœ… **Intelligent Table Selection**: Clustering-based approach avoids redundant tables
- âœ… **Flexible Joins**: FK + column matching + pattern inference
- âœ… **Guaranteed Response**: 100% response rate with fallback chains
- âœ… **Clean Architecture**: PyTorch-style orchestration with modular agents
- âœ… **Docker Support**: Full containerization with visualization

## ğŸ“‹ Architecture

```
User Query
    â†“
[1] Vector Search (1 Crore â†’ Top 30 tables)      ~400ms
    â†“
[2] Table Clustering (30 â†’ Semantic groups)      ~15ms
    â†“
[3] Table Selector (Groups â†’ Best 1/2/3 tables)  ~250ms
    â†“
[4] Schema Packager (Collect metadata)           ~150ms
    â†“
[5] SQL Generator (LLM - temporary)              ~1200ms
    â†“
[6] Validator (Auto-repair)                      ~300ms
    â†“
[7] Executor (Run + format)                      ~200ms

Total: ~2.5 seconds per query
```

## ğŸ—ï¸ Project Structure

```
flash-ops/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/              # Modular pipeline agents
â”‚   â”‚   â”œâ”€â”€ schema_scout.py           # Stage 1: Vector search
â”‚   â”‚   â”œâ”€â”€ table_clustering.py       # Stage 2: Clustering
â”‚   â”‚   â”œâ”€â”€ table_selector.py         # Stage 3: Table selection
â”‚   â”‚   â”œâ”€â”€ schema_packager.py        # Stage 4: Schema collection
â”‚   â”‚   â”œâ”€â”€ sql_generator.py          # Stage 5: SQL generation
â”‚   â”‚   â””â”€â”€ quality_inspector.py      # Stage 6-7: Validation + execution
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestration/       # PyTorch-style handlers
â”‚   â”‚   â”œâ”€â”€ query_pipeline.py         # Main query orchestrator
â”‚   â”‚   â””â”€â”€ embedding_pipeline.py     # Embedding generation orchestrator
â”‚   â”‚
â”‚   â”œâ”€â”€ services/            # External integrations
â”‚   â”‚   â”œâ”€â”€ mongo_client.py           # MongoDB client
â”‚   â”‚   â”œâ”€â”€ vector_store.py           # FAISS operations
â”‚   â”‚   â””â”€â”€ db_client.py              # DuckDB client
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                 # Clean FastAPI routes
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ query.py              # Query endpoint
â”‚   â”‚       â””â”€â”€ embeddings.py         # Embeddings endpoint
â”‚   â”‚
â”‚   â”œâ”€â”€ models/              # Pydantic models
â”‚   â”‚   â”œâ”€â”€ requests.py
â”‚   â”‚   â””â”€â”€ responses.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â””â”€â”€ main.py              # FastAPI app
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ embeddings/          # FAISS index + metadata
â”‚   â”œâ”€â”€ exports/             # CSV exports
â”‚   â””â”€â”€ logs/                # Application logs
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ visualize.ipynb      # Visualization tools
â”‚
â””â”€â”€ requirements.txt
```

## ğŸš¦ Quick Start

### 1. Installation

```bash
# Clone repository
cd flash-ops

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment file
cp .env.example .env
```

### 2. Configure Environment

Edit `.env` file:
```bash
# MongoDB connection
MONGO_URI=mongodb://localhost:27017
MONGO_DB_ID=6919f70d1e144e4ea1b53ff4

# OpenAI API Key (temporary)
OPENAI_API_KEY=your-key-here
```

### 3. Generate Embeddings

```bash
# Start the API server
uvicorn app.main:app --reload

# In another terminal, generate embeddings
curl -X POST "http://localhost:8000/api/v1/embeddings/generate/default"
```

### 4. Run Queries

```bash
# Query endpoint
curl -X POST "http://localhost:8000/api/v1/query/" \
  -H "Content-Type: application/json" \
  -d '{"query": "show all active employees"}'
```

## ğŸ³ Docker Setup

```bash
# Start all services (API + MongoDB + Jupyter)
cd docker
docker-compose up -d

# View logs
docker-compose logs -f app

# Stop services
docker-compose down
```

**Services:**
- API: http://localhost:8000
- Jupyter: http://localhost:8888
- MongoDB: localhost:27017

## ğŸ“Š API Endpoints

### Query Processing

**POST** `/api/v1/query/`

```json
{
  "query": "emp id 111 from IT dept how much sales in May 2025"
}
```

**Response:**
```json
{
  "status": "success",
  "query": "...",
  "tables_used": ["employee_master", "sales_transaction", "department_master"],
  "tier": 3,
  "row_count": 1,
  "result": [{"total_sales": 8000.00}],
  "sql_generated": "SELECT SUM(s1.amount) as total_sales FROM...",
  "execution_time_ms": 2341,
  "confidence": 0.883
}
```

### Generate Embeddings

**POST** `/api/v1/embeddings/generate`

```json
{
  "db_id": "6919f70d1e144e4ea1b53ff4",
  "force_regenerate": false
}
```

### Health Check

**GET** `/health`

```json
{
  "status": "healthy",
  "version": "1.0.0",
  "faiss_index_loaded": true,
  "mongo_connected": true,
  "duckdb_connected": true
}
```

## ğŸ“ˆ Visualization

Open Jupyter notebook for interactive visualization:

```bash
# With Docker
docker-compose up jupyter
# Navigate to http://localhost:8888

# Or locally
jupyter notebook notebooks/visualize.ipynb
```

**Visualizations include:**
- Table embeddings (PCA/t-SNE)
- Similarity heatmaps
- Clustering results
- Query performance metrics

## âš™ï¸ Configuration

Key settings in `app/config.py`:

```python
# Vector Search
VECTOR_SEARCH_TOP_K = 30              # Tables to retrieve
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # Sentence transformer

# Clustering
CLUSTERING_SIMILARITY_THRESHOLD = 0.75  # Cluster threshold

# Table Selection
SINGLE_TABLE_SCORE_GAP = 0.2          # Gap to use single table
MAX_TABLES_PER_QUERY = 3              # Hard limit

# Results
MAX_RESULT_ROWS_IN_RESPONSE = 10      # JSON vs CSV threshold
```

## ğŸ§ª Testing

```bash
# Run sample queries
python -m pytest tests/

# Or test via API
curl -X POST "http://localhost:8000/api/v1/query/" \
  -H "Content-Type: application/json" \
  -d '{"query": "count all active users"}'
```

## ğŸ“ Logging

Logs are written to `data/logs/` with component-specific files:
- `schema_scout_*.log`
- `table_clustering_*.log`
- `query_pipeline_*.log`

## ğŸ”§ Development

### Adding New Agents

1. Create agent in `app/agents/`
2. Add to orchestration in `app/orchestration/query_pipeline.py`
3. Update factory functions

### Modifying Pipeline

Orchestrator uses PyTorch-style handler pattern:
```python
# app/orchestration/query_pipeline.py
def process(self, query: str):
    # Stage 1
    tables = self.schema_scout.search_tables(query)

    # Stage 2
    clusters = self.table_clustering.cluster_tables(tables)

    # Continue...
```

## ğŸš§ Roadmap

- [ ] **Phase 1**: Remove LLM dependency (replace with offline model)
- [ ] **Phase 2**: Add learning system (cache successful patterns)
- [ ] **Phase 3**: Improved query understanding
- [ ] **Phase 4**: Performance optimization (sub-2s response)

## ğŸ“„ License

MIT License

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch
3. Submit pull request

## ğŸ“ Support

For issues and questions:
- GitHub Issues: [Create issue](https://github.com/yourrepo/flash-ops/issues)
- Documentation: See `plan.md` for detailed architecture

---

**Built with â¤ï¸ using FastAPI, FAISS, and clean architecture principles**
